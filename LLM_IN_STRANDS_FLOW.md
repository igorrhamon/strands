# ğŸ¤– Onde a LLM (Ollama) Entra no Fluxo do Strands

## ğŸ“š VisÃ£o Geral

A LLM (Ollama) entra em **3 pontos crÃ­ticos** do pipeline do Strands:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    PIPELINE STRANDS                             â”‚
â”‚                                                                 â”‚
â”‚  1. AlertCollector â†’ 2. Normalizer â†’ 3. Correlator â†’ ...       â”‚
â”‚                                                                 â”‚
â”‚     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚     â”‚                                                      â”‚   â”‚
â”‚     â”‚  ğŸ¤– LLM ENTRA EM 3 PONTOS:                          â”‚   â”‚
â”‚     â”‚                                                      â”‚   â”‚
â”‚     â”‚  1ï¸âƒ£ EMBEDDING AGENT (GeraÃ§Ã£o de Vetores)           â”‚   â”‚
â”‚     â”‚     â””â”€ Converte texto em vetor semÃ¢ntico            â”‚   â”‚
â”‚     â”‚     â””â”€ Modelo: nomic-embed-text                     â”‚   â”‚
â”‚     â”‚                                                      â”‚   â”‚
â”‚     â”‚  2ï¸âƒ£ DECISION ENGINE (AnÃ¡lise de Contexto)          â”‚   â”‚
â”‚     â”‚     â””â”€ Analisa contexto e gera recomendaÃ§Ãµes       â”‚   â”‚
â”‚     â”‚     â””â”€ Modelo: mistral, llama2, etc                â”‚   â”‚
â”‚     â”‚                                                      â”‚   â”‚
â”‚     â”‚  3ï¸âƒ£ REPORT AGENT (GeraÃ§Ã£o de RelatÃ³rios)           â”‚   â”‚
â”‚     â”‚     â””â”€ Gera explicaÃ§Ãµes legÃ­veis para humanos       â”‚   â”‚
â”‚     â”‚     â””â”€ Modelo: mistral, llama2, etc                â”‚   â”‚
â”‚     â”‚                                                      â”‚   â”‚
â”‚     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## 1ï¸âƒ£ LLM no EMBEDDING AGENT

### **FunÃ§Ã£o: GeraÃ§Ã£o de Embeddings (Vetores SemÃ¢nticos)**

```python
# Modelo: nomic-embed-text (384 dimensÃµes)
# FunÃ§Ã£o: Converter texto em vetor numÃ©rico

class EmbeddingAgent:
    async def search_similar(self, alert_description: str):
        """
        Usa LLM para gerar embedding do alerta
        """
        
        # ENTRADA: Texto do alerta
        alert_text = """
        Database connection timeout causing checkout failures.
        500 customers affected, revenue loss $5k/min.
        """
        
        # ğŸ¤– LLM AQUI: Ollama gera embedding
        embedding_vector = await ollama.embed(
            model="nomic-embed-text:latest",
            input=alert_text
        )
        # SAÃDA: [0.156, -0.432, 0.789, ..., 0.234]  (384 nÃºmeros)
        
        # Usa vetor para buscar similares no Qdrant
        similar_results = await qdrant.search(
            vector=embedding_vector,
            top_k=5
        )
        
        return similar_results
```

### **Fluxo Detalhado**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ EMBEDDING AGENT                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Entrada:
â”œâ”€ Alert Text: "Database connection timeout..."
â””â”€ Service: "payment-api"

        â†“

ğŸ¤– LLM (Ollama - nomic-embed-text)
â”œâ”€ POST http://localhost:11434/api/embed
â”œâ”€ Input: "Database connection timeout..."
â””â”€ Output: [0.156, -0.432, 0.789, ..., 0.234]

        â†“

Qdrant Search:
â”œâ”€ Vector: [0.156, -0.432, 0.789, ..., 0.234]
â”œâ”€ Top K: 5
â””â”€ Results: 3 similar incidents

        â†“

SaÃ­da:
â”œâ”€ SimilarityResult 1: 0.96 similarity
â”œâ”€ SimilarityResult 2: 0.88 similarity
â””â”€ SimilarityResult 3: 0.82 similarity
```

### **Quando Ã© Usado**

- âœ… Quando um novo alerta chega
- âœ… Para buscar incidentes similares no histÃ³rico
- âœ… Para encontrar resoluÃ§Ãµes anteriores

### **Modelo Usado**

```
Modelo: nomic-embed-text:latest
DimensÃµes: 384
Velocidade: ~100ms por embedding
Uso: Busca semÃ¢ntica
```

---

## 2ï¸âƒ£ LLM no DECISION ENGINE

### **FunÃ§Ã£o: AnÃ¡lise de Contexto e RecomendaÃ§Ã£o**

```python
# Modelo: mistral, llama2, neural-chat, etc
# FunÃ§Ã£o: Analisar contexto e gerar recomendaÃ§Ã£o

class DecisionEngine:
    async def make_decision(
        self,
        cluster: AlertCluster,
        metrics: MetricsAnalysisResult,
        graph: GraphContext,
        similar: SimilarityResult
    ) -> Decision:
        """
        Usa LLM para analisar contexto e gerar decisÃ£o
        """
        
        # ConstrÃ³i prompt com todo o contexto
        prompt = f"""
        VocÃª Ã© um especialista em resoluÃ§Ã£o de incidentes de infraestrutura.
        
        ALERTA ATUAL:
        - ServiÃ§o: {cluster.service}
        - Severidade: {cluster.severity}
        - DescriÃ§Ã£o: {cluster.description}
        
        ANÃLISE DE MÃ‰TRICAS:
        - TendÃªncia: {metrics.trend}
        - Anomalias: {metrics.anomalies}
        - ConfianÃ§a: {metrics.confidence}
        
        CONTEXTO DE DEPENDÃŠNCIAS:
        - ServiÃ§os dependentes: {graph.dependent_services}
        - HistÃ³rico de falhas: {graph.failure_history}
        
        INCIDENTES SIMILARES ENCONTRADOS:
        - Mais similar (96%): {similar[0].source_text}
          ResoluÃ§Ã£o anterior: {similar[0].previous_action}
          Tempo: {similar[0].resolution_time}
        
        Com base em toda essa anÃ¡lise, qual Ã© a melhor aÃ§Ã£o a tomar?
        Justifique sua recomendaÃ§Ã£o.
        """
        
        # ğŸ¤– LLM AQUI: Ollama analisa contexto
        response = await ollama.generate(
            model="mistral:latest",
            prompt=prompt,
            stream=False
        )
        
        # SAÃDA: RecomendaÃ§Ã£o da LLM
        llm_recommendation = response.response
        
        # Exemplo de resposta:
        # "Com base na anÃ¡lise, recomendo INCREASE_POOL_SIZE
        #  porque:
        #  1. 96% similar ao incidente INC0112345
        #  2. MÃ©trica mostra pool exhausted
        #  3. ResoluÃ§Ã£o anterior funcionou em 15 min
        #  ConfianÃ§a: 95%"
        
        # Converte resposta em Decision
        decision = Decision(
            action=parse_action(llm_recommendation),
            confidence=parse_confidence(llm_recommendation),
            reasoning=llm_recommendation,
            evidence={
                "metrics": metrics,
                "graph": graph,
                "similar": similar
            }
        )
        
        return decision
```

### **Fluxo Detalhado**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ DECISION ENGINE                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Entrada:
â”œâ”€ Alert Cluster: {service, severity, description}
â”œâ”€ Metrics: {trend, anomalies, confidence}
â”œâ”€ Graph Context: {dependent_services, failure_history}
â””â”€ Similar Incidents: {3 incidentes similares}

        â†“

ğŸ¤– LLM (Ollama - mistral)
â”œâ”€ POST http://localhost:11434/api/generate
â”œâ”€ Prompt: "VocÃª Ã© especialista em incidentes..."
â”‚          "Alerta: Database connection timeout..."
â”‚          "MÃ©tricas: Trend=UP, Pool=Exhausted..."
â”‚          "Similar (96%): INCREASE_POOL_SIZE..."
â””â”€ Output: "Recomendo INCREASE_POOL_SIZE porque..."

        â†“

Parsing:
â”œâ”€ Action: INCREASE_POOL_SIZE
â”œâ”€ Confidence: 0.95
â””â”€ Reasoning: "96% similar ao INC0112345..."

        â†“

SaÃ­da:
â””â”€ Decision {
     action: "INCREASE_POOL_SIZE",
     confidence: 0.95,
     reasoning: "..."
   }
```

### **Quando Ã© Usado**

- âœ… Quando precisa analisar contexto complexo
- âœ… Para gerar recomendaÃ§Ãµes baseadas em mÃºltiplas fontes
- âœ… Para justificar decisÃµes para humanos
- âœ… Quando confianÃ§a de regras Ã© baixa (< 0.7)

### **Modelos DisponÃ­veis**

```
mistral:latest       - Bom balanÃ§o velocidade/qualidade
llama2:latest        - Mais preciso, mais lento
neural-chat:latest   - Otimizado para chat
dolphin-mixtral      - Muito bom para anÃ¡lise
```

---

## 3ï¸âƒ£ LLM no REPORT AGENT

### **FunÃ§Ã£o: GeraÃ§Ã£o de RelatÃ³rios LegÃ­veis**

```python
# Modelo: mistral, llama2, etc
# FunÃ§Ã£o: Gerar explicaÃ§Ãµes em linguagem natural

class ReportAgent:
    async def generate_report(
        self,
        decision: Decision,
        cluster: AlertCluster,
        metrics: MetricsAnalysisResult,
        similar: SimilarityResult
    ) -> str:
        """
        Usa LLM para gerar relatÃ³rio legÃ­vel para humanos
        """
        
        prompt = f"""
        Gere um relatÃ³rio executivo sobre este incidente para um analista.
        
        INCIDENTE:
        - ServiÃ§o: {cluster.service}
        - Severidade: {cluster.severity}
        - DescriÃ§Ã£o: {cluster.description}
        
        ANÃLISE:
        - Causa provÃ¡vel: {metrics.root_cause}
        - Impacto: {metrics.impact}
        - DuraÃ§Ã£o estimada: {metrics.estimated_duration}
        
        DECISÃƒO RECOMENDADA:
        - AÃ§Ã£o: {decision.action}
        - ConfianÃ§a: {decision.confidence}
        - Justificativa: {decision.reasoning}
        
        HISTÃ“RICO SIMILAR:
        - Incidente anterior: {similar[0].source_text}
        - ResoluÃ§Ã£o: {similar[0].previous_action}
        - Tempo para resolver: {similar[0].resolution_time}
        
        Gere um relatÃ³rio profissional em portuguÃªs que:
        1. Resuma o problema
        2. Explique a causa
        3. Recomende a aÃ§Ã£o
        4. Cite o histÃ³rico similar
        5. Indique prÃ³ximos passos
        """
        
        # ğŸ¤– LLM AQUI: Ollama gera relatÃ³rio
        response = await ollama.generate(
            model="mistral:latest",
            prompt=prompt,
            stream=False
        )
        
        # SAÃDA: RelatÃ³rio legÃ­vel
        report = response.response
        
        # Exemplo de saÃ­da:
        # """
        # RELATÃ“RIO DE INCIDENTE - INC0123456
        #
        # RESUMO:
        # O serviÃ§o payment-api estÃ¡ indisponÃ­vel devido a esgotamento
        # do pool de conexÃµes do banco de dados.
        #
        # CAUSA:
        # O tamanho do pool foi reduzido de 100 para 50 conexÃµes durante
        # manutenÃ§Ã£o, e nÃ£o foi restaurado. AlÃ©m disso, novas regras de
        # validaÃ§Ã£o de pagamento aumentaram o tempo de conexÃ£o.
        #
        # AÃ‡ÃƒO RECOMENDADA:
        # Aumentar o pool de conexÃµes de 50 para 100 e fazer rollback
        # das regras de validaÃ§Ã£o.
        #
        # HISTÃ“RICO SIMILAR:
        # Este problema Ã© 96% similar ao incidente INC0112345 de 1 semana
        # atrÃ¡s, que foi resolvido em 15 minutos com a mesma aÃ§Ã£o.
        #
        # PRÃ“XIMOS PASSOS:
        # 1. Executar aÃ§Ã£o recomendada
        # 2. Monitorar taxa de sucesso de checkout
        # 3. Validar mÃ©tricas de banco de dados
        # 4. Fazer rollback completo das mudanÃ§as
        # """
        
        return report
```

### **Fluxo Detalhado**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ REPORT AGENT                                                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Entrada:
â”œâ”€ Decision: {action, confidence, reasoning}
â”œâ”€ Cluster: {service, severity, description}
â”œâ”€ Metrics: {root_cause, impact, duration}
â””â”€ Similar: {3 incidentes similares}

        â†“

ğŸ¤– LLM (Ollama - mistral)
â”œâ”€ POST http://localhost:11434/api/generate
â”œâ”€ Prompt: "Gere um relatÃ³rio executivo..."
â”‚          "ServiÃ§o: payment-api..."
â”‚          "AÃ§Ã£o: INCREASE_POOL_SIZE..."
â””â”€ Output: "RELATÃ“RIO DE INCIDENTE..."

        â†“

FormataÃ§Ã£o:
â”œâ”€ Resumo
â”œâ”€ Causa
â”œâ”€ AÃ§Ã£o Recomendada
â”œâ”€ HistÃ³rico Similar
â””â”€ PrÃ³ximos Passos

        â†“

SaÃ­da:
â””â”€ RelatÃ³rio em Markdown/HTML
```

### **Quando Ã© Usado**

- âœ… Para notificar analistas humanos
- âœ… Para criar tickets no ServiceNow
- âœ… Para enviar alertas por email/Slack
- âœ… Para documentar incidentes
- âœ… Para treinamento e aprendizado

### **SaÃ­da Exemplo**

```markdown
# RELATÃ“RIO DE INCIDENTE - INC0123456

## RESUMO
O serviÃ§o payment-api estÃ¡ indisponÃ­vel devido a esgotamento do pool 
de conexÃµes do banco de dados.

## CAUSA
O tamanho do pool foi reduzido de 100 para 50 conexÃµes durante 
manutenÃ§Ã£o, e nÃ£o foi restaurado. AlÃ©m disso, novas regras de 
validaÃ§Ã£o aumentaram o tempo de conexÃ£o.

## AÃ‡ÃƒO RECOMENDADA
Aumentar pool de 50 para 100 e fazer rollback das validaÃ§Ãµes.

## HISTÃ“RICO SIMILAR
96% similar ao INC0112345 (resolvido em 15 minutos)

## PRÃ“XIMOS PASSOS
1. Executar aÃ§Ã£o recomendada
2. Monitorar taxa de sucesso
3. Validar mÃ©tricas de DB
```

---

## ğŸ”„ Fluxo Completo com LLM

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 1. ALERTA CHEGA                                              â”‚
â”‚    â””â”€ "Database connection timeout"                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 2. ALERT COLLECTOR                                           â”‚
â”‚    â””â”€ Coleta de Prometheus/Grafana/ServiceNow               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 3. ALERT NORMALIZER                                          â”‚
â”‚    â””â”€ Valida e padroniza                                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 4. ALERT CORRELATOR                                          â”‚
â”‚    â””â”€ Agrupa alertas relacionados                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚                 â”‚
        â–¼                 â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ METRICS      â”‚   â”‚ GRAPH AGENT      â”‚
â”‚ ANALYSIS     â”‚   â”‚ (Neo4j)          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚                 â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
                 â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ ğŸ¤– EMBEDDING AGENT         â”‚
    â”‚ (LLM - nomic-embed-text)   â”‚
    â”‚                            â”‚
    â”‚ 1. Gera embedding do alertaâ”‚
    â”‚ 2. Busca similares em      â”‚
    â”‚    Qdrant                  â”‚
    â”‚ 3. Retorna 3 similares     â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
                 â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ ğŸ¤– DECISION ENGINE         â”‚
    â”‚ (LLM - mistral/llama2)     â”‚
    â”‚                            â”‚
    â”‚ 1. Analisa contexto        â”‚
    â”‚ 2. Consulta histÃ³rico      â”‚
    â”‚ 3. Gera recomendaÃ§Ã£o       â”‚
    â”‚ 4. Retorna Decision        â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
                 â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ HUMAN REVIEW               â”‚
    â”‚ (se confianÃ§a < 70%)       â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
                 â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ ğŸ¤– REPORT AGENT            â”‚
    â”‚ (LLM - mistral/llama2)     â”‚
    â”‚                            â”‚
    â”‚ 1. Gera relatÃ³rio          â”‚
    â”‚ 2. Cria notificaÃ§Ã£o        â”‚
    â”‚ 3. Atualiza ServiceNow     â”‚
    â”‚ 4. Envia email/Slack       â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
                 â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ SAÃDA                      â”‚
    â”‚ - Ticket criado            â”‚
    â”‚ - NotificaÃ§Ã£o enviada      â”‚
    â”‚ - AÃ§Ã£o executada           â”‚
    â”‚ - RelatÃ³rio gerado         â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“Š Resumo: Onde LLM Entra

| Ponto | Agente | Modelo | FunÃ§Ã£o | Entrada | SaÃ­da |
|-------|--------|--------|--------|---------|-------|
| **1ï¸âƒ£** | EmbeddingAgent | nomic-embed-text | Gera embedding | Texto do alerta | Vetor 384-dim |
| **2ï¸âƒ£** | DecisionEngine | mistral/llama2 | Analisa contexto | Contexto completo | RecomendaÃ§Ã£o |
| **3ï¸âƒ£** | ReportAgent | mistral/llama2 | Gera relatÃ³rio | Decision + contexto | RelatÃ³rio legÃ­vel |

---

## ğŸ¯ Exemplo PrÃ¡tico Completo

### **CenÃ¡rio: Novo Alerta de Timeout**

```
TEMPO: 12:00:00

1. ALERTA CHEGA
   â””â”€ "Database connection timeout"

2. EMBEDDING AGENT
   â”œâ”€ ğŸ¤– Ollama (nomic-embed-text)
   â”œâ”€ Input: "Database connection timeout..."
   â”œâ”€ Output: [0.156, -0.432, 0.789, ..., 0.234]
   â”œâ”€ Qdrant Search: Top 3 similares
   â””â”€ Retorna: 3 incidentes similares (96%, 88%, 82%)

3. DECISION ENGINE
   â”œâ”€ ğŸ¤– Ollama (mistral)
   â”œâ”€ Input: "VocÃª Ã© especialista em incidentes..."
   â”‚         "Alerta: Database timeout..."
   â”‚         "Similar (96%): INCREASE_POOL_SIZE..."
   â”œâ”€ Output: "Recomendo INCREASE_POOL_SIZE porque..."
   â””â”€ Retorna: Decision(action=INCREASE_POOL_SIZE, confidence=0.95)

4. HUMAN REVIEW
   â”œâ”€ ConfianÃ§a: 0.95 (> 0.7)
   â””â”€ Status: APROVADO (sem revisÃ£o)

5. REPORT AGENT
   â”œâ”€ ğŸ¤– Ollama (mistral)
   â”œâ”€ Input: "Gere um relatÃ³rio executivo..."
   â”‚         "ServiÃ§o: payment-api..."
   â”‚         "AÃ§Ã£o: INCREASE_POOL_SIZE..."
   â”œâ”€ Output: "RELATÃ“RIO DE INCIDENTE..."
   â””â”€ Retorna: RelatÃ³rio em Markdown

6. SAÃDA
   â”œâ”€ Ticket criado no ServiceNow
   â”œâ”€ Email enviado para analista
   â”œâ”€ Slack notificado
   â”œâ”€ AÃ§Ã£o executada
   â””â”€ RelatÃ³rio documentado

TEMPO TOTAL: ~2 segundos
```

---

## ğŸ” ConfiguraÃ§Ã£o do Ollama

### **.env**

```bash
# Ollama
OLLAMA_URL=http://localhost:11434

# Modelos
OLLAMA_EMBEDDING_MODEL=nomic-embed-text:latest
OLLAMA_DECISION_MODEL=mistral:latest
OLLAMA_REPORT_MODEL=mistral:latest

# Timeouts
OLLAMA_EMBEDDING_TIMEOUT=30s
OLLAMA_DECISION_TIMEOUT=60s
OLLAMA_REPORT_TIMEOUT=60s
```

### **docker-compose.yml**

```yaml
services:
  ollama:
    image: ollama/ollama:latest
    ports:
      - "11434:11434"
    environment:
      - OLLAMA_NUM_PARALLEL=4
      - OLLAMA_NUM_THREAD=8
    volumes:
      - ollama-data:/root/.ollama
    command: serve

volumes:
  ollama-data:
```

### **Inicializar Modelos**

```bash
# Pull modelos
ollama pull nomic-embed-text:latest
ollama pull mistral:latest
ollama pull llama2:latest

# Verificar
curl http://localhost:11434/api/tags
```

---

## ğŸ“ˆ MÃ©tricas de LLM

### **Prometheus Metrics**

```
# LatÃªncia de embedding
strands_llm_embedding_seconds = 0.123

# LatÃªncia de decisÃ£o
strands_llm_decision_seconds = 2.456

# Taxa de sucesso
strands_llm_success_rate = 0.98

# Tokens processados
strands_llm_tokens_processed = 12345

# Custo (se usar API externa)
strands_llm_cost_usd = 0.45
```

### **Jaeger Traces**

```
Trace: decision_generation_abc123
â”œâ”€ EmbeddingAgent
â”‚  â”œâ”€ Ollama.embed (123ms)
â”‚  â””â”€ Qdrant.search (89ms)
â”œâ”€ DecisionEngine
â”‚  â”œâ”€ Ollama.generate (2456ms)
â”‚  â””â”€ Parse response (12ms)
â”œâ”€ ReportAgent
â”‚  â”œâ”€ Ollama.generate (1234ms)
â”‚  â””â”€ Format markdown (8ms)
â””â”€ Total: 3.9s
```

---

## ğŸš€ Resumo

**A LLM (Ollama) entra em 3 pontos crÃ­ticos:**

1. **ğŸ¤– EMBEDDING AGENT** (nomic-embed-text)
   - Converte texto em vetor semÃ¢ntico
   - Busca incidentes similares no Qdrant
   - ~100ms por embedding

2. **ğŸ¤– DECISION ENGINE** (mistral/llama2)
   - Analisa contexto complexo
   - Gera recomendaÃ§Ãµes baseadas em histÃ³rico
   - ~2-3 segundos por decisÃ£o

3. **ğŸ¤– REPORT AGENT** (mistral/llama2)
   - Gera relatÃ³rios legÃ­veis para humanos
   - Cria notificaÃ§Ãµes e tickets
   - ~1-2 segundos por relatÃ³rio

**Total: ~4-5 segundos do alerta Ã  decisÃ£o final**

A LLM nÃ£o substitui as regras determinÃ­sticas, mas as **complementa** com anÃ¡lise semÃ¢ntica e contexto!
